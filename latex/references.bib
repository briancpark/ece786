@inproceedings{hyquas,
author = {Zhang, Chen and Song, Zeyu and Wang, Haojie and Rong, Kaiyuan and Zhai, Jidong},
title = {HyQuas: Hybrid Partitioner Based Quantum Circuit Simulation System on GPU},
year = {2021},
isbn = {9781450383356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447818.3460357},
doi = {10.1145/3447818.3460357},
abstract = {Quantum computing has shown its strong potential in solving certain important problems. Due to the intrinsic limitations of current real quantum computers, quantum circuit simulation still plays an important role in both research and development of quantum computing. GPU-based quantum circuit simulation has been explored due to GPU's high computation capability. Despite previous efforts, existing quantum circuit simulation systems usually rely on a single method to improve poor data locality caused by complex quantum entanglement. However, we observe that existing simulation methods show significantly different performance for different circuit patterns. The optimal performance cannot be obtained only with any single method.To address these challenges, we propose HyQuas, a textbf{Hy}brid partitioner based textbf{Qua}ntum circuit textbf{S}imulation system on GPU, which can automatically select the suitable simulation method for different parts of a given quantum circuit according to its pattern. Moreover, to make better support for HyQuas, we also propose two highly optimized methods, OShareMem and TransMM, as optional choices of HyQuas. We further propose a GPU-centric communication pipelining approach for effective distributed simulation. Experimental results show that HyQuas can achieve up to 10.71 x speedup on a single GPU and 227 x speedup on a GPU cluster over state-of-the-art quantum circuit simulation systems.},
booktitle = {Proceedings of the ACM International Conference on Supercomputing},
pages = {443–454},
numpages = {12},
keywords = {simulation, GPU computing, quantum computing},
location = {Virtual Event, USA},
series = {ICS '21}
}

@inproceedings{cachebypassing,
author = {Li, Chao and Song, Shuaiwen Leon and Dai, Hongwen and Sidelnik, Albert and Hari, Siva Kumar Sastry and Zhou, Huiyang},
title = {Locality-Driven Dynamic GPU Cache Bypassing},
year = {2015},
isbn = {9781450335591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2751205.2751237},
doi = {10.1145/2751205.2751237},
abstract = {This paper presents novel cache optimizations for massively parallel, throughput-oriented architectures like GPUs. L1 data caches (L1 D-caches) are critical resources for providing high-bandwidth and low-latency data accesses. However, the high number of simultaneous requests from single-instruction multiple-thread (SIMT) cores makes the limited capacity of L1 D-caches a performance and energy bottleneck, especially for memory-intensive applications. We observe that the memory access streams to L1 D-caches for many applications contain a significant amount of requests with low reuse, which greatly reduce the cache efficacy. Existing GPU cache management schemes are either based on conditional/reactive solutions or hit-rate based designs specifically developed for CPU last level caches, which can limit overall performance.To overcome these challenges, we propose an efficient locality monitoring mechanism to dynamically filter the access stream on cache insertion such that only the data with high reuse and short reuse distances are stored in the L1 D-cache. Specifically, we present a design that integrates locality filtering based on reuse characteristics of GPU workloads into the decoupled tag store of the existing L1 D-cache through simple and cost-effective hardware extensions. Results show that our proposed design can dramatically reduce cache contention and achieve up to 56.8% and an average of 30.3% performance improvement over the baseline architecture, for a range of highly-optimized cache-unfriendly applications with minor area overhead and better energy efficiency. Our design also significantly outperforms the state-of-the-art CPU and GPU bypassing schemes (especially for irregular applications), without generating extra L2 and DRAM level contention.},
booktitle = {Proceedings of the 29th ACM on International Conference on Supercomputing},
pages = {67–77},
numpages = {11},
keywords = {locality, cache bypassing, gpu architecture optimization},
location = {Newport Beach, California, USA},
series = {ICS '15}
}